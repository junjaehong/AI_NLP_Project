{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f58be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import mysql.connector\n",
    "import pymysql\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from konlpy.tag import Okt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class newsScrap :\n",
    "    \n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"}\n",
    "    \n",
    "    def __init__(self, company):\n",
    "        self.company = company\n",
    "        self.titles = []\n",
    "        self.dates = []\n",
    "        self.articles = []\n",
    "        self.article_urls = []\n",
    "        self.press_companies = []\n",
    "        \n",
    "    def createNewsLinks(self):\n",
    "        start_point = 1\n",
    "        while True :\n",
    "            #pd => 4=1일 , 2= 1달\n",
    "            url = 'https://search.naver.com/search.naver?where=news&query='+ str(self.company) + '&sm=tab_opt&sort=1&photo=0&field=0&pd=4&start=' + str(start_point)\n",
    "            web = requests.get(url).content\n",
    "            source = BeautifulSoup(web, 'html.parser')\n",
    "            \n",
    "            for urls in source.find_all('a', {'class' : \"info\"}):\n",
    "                if urls[\"href\"].startswith(\"https://n.news.naver.com\"):\n",
    "                    self.article_urls.append(urls[\"href\"])\n",
    "#                     print(urls[\"href\"])\n",
    "            is_last_page = source.find('a',{'class':'btn_next'}).get('aria-disabled')\n",
    "            if is_last_page == \"true\":\n",
    "#                 print(\"last page\")\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "                start_point += 10\n",
    "                \n",
    "                    \n",
    "    def newsDataScrap(self):\n",
    "        for url in self.article_urls:\n",
    "            try:\n",
    "                web_news = requests.get(url, headers=self.headers).content\n",
    "                source_news = BeautifulSoup(web_news, 'html.parser')\n",
    "\n",
    "                title = source_news.find('h2', {'class' : 'media_end_head_headline'}).get_text()\n",
    "\n",
    "                date = source_news.find('span', {'class' : 'media_end_head_info_datestamp_time'}).get_text()\n",
    "\n",
    "                article = source_news.find('div', {'id' : 'newsct_article'}).get_text()\n",
    "                article = article.replace(\"\\n\", \"\")\n",
    "                article = article.replace(\"// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}\", \"\")\n",
    "                article = article.replace(\"동영상 뉴스       \", \"\")\n",
    "                article = article.replace(\"동영상 뉴스\", \"\")\n",
    "                article = article.strip()\n",
    "\n",
    "                press_company = source_news.find('em', {'class':'media_end_linked_more_point'}).get_text()\n",
    "                \n",
    "            \n",
    "                self.titles.append(title)\n",
    "                self.dates.append(date)\n",
    "                self.articles.append(article)\n",
    "                self.press_companies.append(press_company)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            \n",
    "    def saveExcel(self):\n",
    "        article_df = pd.DataFrame({'Title':self.titles, \n",
    "                                   'Date':self.dates, \n",
    "                                   'Article':self.articles, \n",
    "                                   'URL':self.article_urls, \n",
    "                                   'PressCompany':self.press_companies})\n",
    "        article_df.to_excel('{}_{}.xlsx'.format(self.company, datetime.now().strftime('%y%m%d_%H%M')), index=False)\n",
    "        \n",
    "        \n",
    "    def dbUpdater(self):\n",
    "        config = {'user': '',\n",
    "                'password': '',\n",
    "                'host': '',\n",
    "                'database': '',\n",
    "                'raise_on_warnings': True}\n",
    "        \n",
    "        cnn = pymysql.connect(host='', \n",
    "                              user='', password='',db='')\n",
    "        querys_data = []\n",
    "        \n",
    "        cur = cnn.cursor()\n",
    "        \n",
    "        #테이블에 맞춰 항목 추가 예정 \n",
    "        for i in range(len(self.article_urls)):\n",
    "            data = (i, self.titles[i], self.articles[i], self.dates[i], self.press_companies[i],self.article_urls[i])\n",
    "            querys_data.append(data)\n",
    "        \n",
    "        query = 'INSERT INTO news_data VALUES (%s, %s, %s, %s, %s, %s)'\n",
    "        #query = \"INSERT INTO test (name, City) VALUES (%s, %s)\"\n",
    "        \n",
    "        for i in querys_data:\n",
    "            cur.execute(query, i)\n",
    "        \n",
    "        cnn.commit()\n",
    "        cur.close()\n",
    "        cnn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2599e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = newsScrap('롯데백화점')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.createNewsLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e527ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda592a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.newsDataScrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.articles[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6760c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dbUpdater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af628535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideArticle(artList):\n",
    "    divideArt = []\n",
    "    for art in artList:\n",
    "        artTemp = art.split('.')\n",
    "        divideArt.append(artTemp)\n",
    "    return divideArt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62120e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ditest = divideArticle(test.articles[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'user': '',\n",
    "            'password': '',\n",
    "            'host': '',\n",
    "            'database': '',\n",
    "            'raise_on_warnings': True}\n",
    "cnn = pymysql.connect(host='', user='', password='',db='')        \n",
    "cur = cnn.cursor()\n",
    "\n",
    "query = 'SELECT idx, news_id, enter_id FROM news_data'\n",
    "query2 = 'SELECT enter_id FROM enterprise_data WHERE enter_name = %s'\n",
    "query3 = 'SELECT * FROM enterprise_data'\n",
    "data = '한화'\n",
    "cur.execute(query)\n",
    "\n",
    "\n",
    "result = cur.fetchall()\n",
    "# print(result)\n",
    "# print(type(result))\n",
    "# print(str(result))\n",
    "# print(type(result))\n",
    "# print(result[0][0])\n",
    "for row in result:\n",
    "     print(row)\n",
    "cur.close()\n",
    "cnn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49910d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "host=os.getenv(\"HOST\")\n",
    "user=os.getenv(\"USER_NAME\")\n",
    "password=os.getenv(\"PASSWORD\")\n",
    "database=os.getenv(\"DATABASE\")\n",
    "cnn = pymysql.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n",
    "cur = cnn.cursor()\n",
    "\n",
    "query = 'SELECT idx, news_id, enter_id FROM news_data'\n",
    "query2 = 'SELECT enter_id FROM enterprise_data WHERE enter_name = %s'\n",
    "query3 = 'SELECT * FROM enterprise_data'\n",
    "data = '한화'\n",
    "cur.execute(query)\n",
    "\n",
    "\n",
    "result = cur.fetchall()\n",
    "# print(result)\n",
    "# print(type(result))\n",
    "# print(str(result))\n",
    "# print(type(result))\n",
    "# print(result[0][0])\n",
    "for row in result:\n",
    "     print(row)\n",
    "cur.close()\n",
    "cnn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca6069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class newsScrap :\n",
    "    \n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"}\n",
    "    \n",
    "    def __init__(self, company):\n",
    "        self.company = company\n",
    "        self.titles = []\n",
    "        self.dates = []\n",
    "        self.articles = []\n",
    "        self.article_urls = []\n",
    "        self.press_companies = []\n",
    "        self.url_full = []\n",
    "        \n",
    "    def createNewsLinks(self):\n",
    "        start_point = 1\n",
    "        while True :\n",
    "            #pd => 4=1일 , 2= 1달\n",
    "            url = 'https://search.naver.com/search.naver?where=news&query='+ str(self.company) + '&sm=tab_opt&sort=1&photo=0&field=0&pd=4&start=' + str(start_point)\n",
    "            web = requests.get(url).content\n",
    "            source = BeautifulSoup(web, 'html.parser')\n",
    "            \n",
    "            for urls in source.find_all('a', {'class' : \"info\"}):\n",
    "                if urls[\"href\"].startswith(\"https://n.news.naver.com\"):\n",
    "                    self.article_urls.append(urls[\"href\"])\n",
    "                    # print(urls[\"href\"])\n",
    "            is_last_page = source.find('a',{'class':'btn_next'}).get('aria-disabled')\n",
    "            if is_last_page == \"true\":\n",
    "                # print(\"last page\")\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "                start_point += 10\n",
    "                \n",
    "                    \n",
    "    def newsDataScrap(self):\n",
    "        for url in self.article_urls:\n",
    "            try:\n",
    "                web_news = requests.get(url, headers=self.headers).content\n",
    "                source_news = BeautifulSoup(web_news, 'html.parser')\n",
    "\n",
    "                title = source_news.find('h2', {'class' : 'media_end_head_headline'}).get_text()\n",
    "\n",
    "                date = source_news.find('span', {'class' : 'media_end_head_info_datestamp_time'}).get_text()\n",
    "\n",
    "                article = source_news.find('div', {'id' : 'newsct_article'}).get_text()\n",
    "                article = article.replace(\"\\n\", \"\")\n",
    "                article = article.replace(\"// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}\", \"\")\n",
    "                article = article.replace(\"동영상 뉴스       \", \"\")\n",
    "                article = article.replace(\"동영상 뉴스\", \"\")\n",
    "                article = article.strip()\n",
    "\n",
    "                press_company = source_news.find('em', {'class':'media_end_linked_more_point'}).get_text()\n",
    "                \n",
    "                \n",
    "                self.titles.append(title)\n",
    "                self.dates.append(date)\n",
    "                self.articles.append(article)\n",
    "                self.press_companies.append(press_company)\n",
    "                self.url_full.append(url)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            \n",
    "    def saveExcel(self):\n",
    "        article_df = pd.DataFrame({'Title':self.titles, \n",
    "                                   'Date':self.dates, \n",
    "                                   'Article':self.articles, \n",
    "                                   'URL':self.url_full, \n",
    "                                   'PressCompany':self.press_companies})\n",
    "        article_df.to_excel('{}_{}.xlsx'.format(self.company, datetime.now().strftime('%y%m%d_%H%M')), index=False)\n",
    "        \n",
    "        \n",
    "    def dbUpdater(self):\n",
    "        load_dotenv()\n",
    "\n",
    "        host=os.getenv(\"HOST\")\n",
    "        user=os.getenv(\"USER_NAME\")\n",
    "        password=os.getenv(\"PASSWORD\")\n",
    "        database=os.getenv(\"DATABASE\")\n",
    "\n",
    "        cnn = pymysql.connect(host=host, user=user, password=password, database=database)\n",
    "        \n",
    "        querys_data = []\n",
    "        \n",
    "        cur = cnn.cursor()\n",
    "        \n",
    "        selectQuery = 'SELECT enter_id FROM enterprise_data WHERE enter_name = %s'\n",
    "        \n",
    "        cur.execute(selectQuery, self.company)\n",
    "        \n",
    "        e_id = cur.fetchall()\n",
    "#         print(self.company)\n",
    "#         print(type(self.company))\n",
    "        \n",
    "        #테이블에 맞춰 항목 추가 예정 \n",
    "        for i in range(len(self.titles)):\n",
    "            data = (self.titles[i], self.articles[i], e_id[0][0], self.url_full[i])\n",
    "            querys_data.append(data)\n",
    "        \n",
    "        query = 'INSERT INTO news_data(news_id, news_doc, enter_id, url) VALUES (%s, %s, %s, %s)'\n",
    "        #query = \"INSERT INTO test (name, City) VALUES (%s, %s)\"\n",
    "        \n",
    "        for i in querys_data:\n",
    "            cur.execute(query, i)\n",
    "        \n",
    "        cnn.commit()\n",
    "        cur.close()\n",
    "        cnn.close()\n",
    "        \n",
    "    def keyWord(self):\n",
    "        keyword_article = ''.join(self.articles)\n",
    "\n",
    "        tokenizer = Okt()\n",
    "        raw_pos_tagged = tokenizer.pos(keyword_article, norm=True, stem=True)\n",
    "\n",
    "        del_list = ['하다', '있다', '되다', '이다', '돼다', '않다', '그렇다', '아니다', '이렇다', '그렇다', '어떻다']\n",
    "\n",
    "        word_cleaned = []\n",
    "        for word in raw_pos_tagged:\n",
    "            if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\", \"Foreign\"]: # Foreign == ”, “ 와 같이 제외되어야할 항목들\n",
    "                if (len(word[0]) != 1) and (word[0] not in del_list): # 한 글자로 이뤄진 단어들을 제외 & 원치 않는 단어들을 제외\n",
    "            # 숫자나 이메일 형식의 단어 제외\n",
    "                    if not re.match(r'^[0-9]*$', word[0]) and not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', word[0]):\n",
    "                        word_cleaned.append(word[0])\n",
    "\n",
    "        word_dic = {}\n",
    "\n",
    "        for word in word_cleaned:\n",
    "            if word not in word_dic:\n",
    "                word_dic[word] = 1 # changed from \"0\" to \"1\"\n",
    "            else:\n",
    "                word_dic[word] += 1\n",
    "\n",
    "        sorted_word_dic = sorted(word_dic.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        for word, count in sorted_word_dic[:50]:\n",
    "            print(\"{0}({1})\".format(word, count), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7df0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = newsScrap('쿠팡')\n",
    "test.createNewsLinks()\n",
    "# print(test.article_urls)\n",
    "# print(len(test.article_urls))\n",
    "test.newsDataScrap()\n",
    "# print(test.titles)\n",
    "# print(len(test.titles))\n",
    "# print(len(test.url_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b75e24f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "플랫폼(389) 쿠팡(334) 기업(289) 회장(212) 한국(206) 국내(183) 시장(161) 대표(151) 투자(122) 네이버(119) 서비스(118) 그룹(117) 규제(116) 사업(114) 밝히다(113) 경제(110) 오다(110) 이상(109) 글로벌(108) 제공(107) 지난(106) 경쟁(106) 같다(105) 늘다(103) 마케팅(102) 온라인(97) 티맵(96) 업계(95) 올해(91) 미디어(91) 가다(90) 정부(89) 사업자(89) 제품(88) 추진(87) 광고(86) 대다(85) 통해(84) 스타트업(83) 따르다(83) 리테일(82) OTT(82) 받다(81) HS(81) 관련(77) 새롭다(75) 소비자(75) 고객(75) 위해(75) 없다(74) "
     ]
    }
   ],
   "source": [
    "test.keyWord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a6394",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dbUpdater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8bf24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
